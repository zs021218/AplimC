#!/usr/bin/env python3
"""
AplimC - å¤šæ¨¡æ€è—»ç±»åˆ†ç±»é¡¹ç›®
ä¸“ä¸ºHDF5æ ¼å¼ä¼˜åŒ–çš„é«˜æ•ˆå¤šæ¨¡æ€æœºå™¨å­¦ä¹ æ¡†æ¶
"""

__version__ = "3.0.0"
__author__ = "Sen Zhang"
__description__ = "å¤šæ¨¡æ€è—»ç±»åˆ†ç±»ç³»ç»Ÿ - æ”¯æŒStokeså‚æ•°ã€è§å…‰ä¿¡å·å’Œå¤šè§†å›¾å›¾åƒï¼ŒåŒ…å«çŸ¥è¯†è’¸é¦å’Œå¹³è¡¡èåˆ"

# æ ¸å¿ƒæ¨¡å—å¯¼å…¥
from . import data
from . import models
from . import training
from . import utils

# ä¸»è¦ç±»å’Œå‡½æ•°å¯¼å…¥
from .data import (
    MultimodalHDF5Dataset,
    create_default_dataloader,
    get_train_transforms,
    get_val_transforms
)

from .models import (
    MultimodalClassifier,
    SignalEncoder,
    ImageEncoder,
    AttentionFusion,
    create_model,
    create_simple_model,
    FeatureMimicryDistillation,
    EnhancedSignalClassifier,
    RelationKnowledgeExtractor,
    AdaptiveAttentionTransfer,
    DistillationLoss
)

from .training import (
    MultimodalTrainer
)

from .utils import (
    ModelConfig,
    get_preset_config,
    count_parameters,
    analyze_model,
    save_model_checkpoint,
    load_model_checkpoint
)

# ç‰ˆæœ¬ä¿¡æ¯
VERSION_INFO = {
    "version": __version__,
    "author": __author__,
    "description": __description__,
    "python_requires": ">=3.8",
    "pytorch_requires": ">=1.8.0",
    "new_features": [
        "çŸ¥è¯†è’¸é¦æ¡†æ¶ (å›¾åƒâ†’ä¿¡å·æ¨¡æ€çŸ¥è¯†è½¬ç§»)",
        "å¹³è¡¡å¤šæ¨¡æ€èåˆç­–ç•¥",
        "å¢å¼ºç‰ˆä¿¡å·ç¼–ç å™¨",
        "è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶",
        "ç‰¹å¾æ¨¡ä»¿å­¦ä¹ "
    ]
}

# é¡¹ç›®ä¿¡æ¯
PROJECT_INFO = {
    "name": "AplimC",
    "full_name": "Algae classification with Polarization, Light and Image Multimodal Classifier",
    "data_format": "HDF5",
    "modalities": ["stokes", "fluorescence", "images"],
    "num_classes": 12,
    "total_samples": 21007,
    "features": {
        "multimodal_fusion": ["concat", "attention", "balanced", "gradient_balanced", "adaptive"],
        "knowledge_distillation": ["feature_mimicry", "relation_knowledge", "attention_transfer"],
        "signal_enhancement": ["multi_scale_conv", "enhanced_encoder", "adaptive_attention"],
        "performance": {
            "image_only": "95.59%",
            "fluorescence_only": "66.96%", 
            "multimodal": "96.41%",
            "distillation_target": "90%+ (signal only)"
        }
    }
}

# å…¬å¼€çš„API
__all__ = [
    # ç‰ˆæœ¬ä¿¡æ¯
    "__version__",
    "__author__", 
    "__description__",
    "VERSION_INFO",
    "PROJECT_INFO",
    
    # æ•°æ®æ¨¡å—
    "MultimodalHDF5Dataset",
    "create_default_dataloader",
    "get_train_transforms",
    "get_val_transforms",
    
    # æ¨¡å‹æ¨¡å—
    "MultimodalClassifier",
    "SignalEncoder",
    "ImageEncoder", 
    "AttentionFusion",
    "create_model",
    "create_simple_model",
    
    # çŸ¥è¯†è’¸é¦æ¨¡å—
    "FeatureMimicryDistillation",
    "EnhancedSignalClassifier", 
    "DistillationLoss",
    "load_pretrained_teacher",
    
    # å¹³è¡¡èåˆæ¨¡å—
    "ModalityBalancer",
    "GradientBalancedFusion", 
    "AdaptiveFusion",
    "CrossModalAttention",
    
    # è®­ç»ƒæ¨¡å—
    "MultimodalTrainer",
    
    # å·¥å…·æ¨¡å—
    "ModelConfig",
    "get_preset_config",
    "count_parameters",
    "analyze_model",
    "save_model_checkpoint", 
    "load_model_checkpoint",
    
    # å­æ¨¡å—
    "data",
    "models", 
    "training",
    "utils"
]


def get_project_info():
    """è·å–é¡¹ç›®ä¿¡æ¯"""
    return PROJECT_INFO.copy()


def get_version_info():
    """è·å–ç‰ˆæœ¬ä¿¡æ¯"""
    return VERSION_INFO.copy()


def quick_start_example():
    """å¿«é€Ÿå¼€å§‹ç¤ºä¾‹ä»£ç """
    example = """
# AplimC å¿«é€Ÿå¼€å§‹ç¤ºä¾‹

import torch
from src import (
    ModelConfig, 
    MultimodalClassifier,
    MultimodalTrainer,
    create_default_dataloader,
    get_preset_config,
    # çŸ¥è¯†è’¸é¦
    FeatureMimicryDistillation,
    DistillationLoss,
    # å¹³è¡¡èåˆ
    ModalityBalancer
)

# === æ–¹æ¡ˆ1: æ ‡å‡†å¤šæ¨¡æ€åˆ†ç±» ===
print("æ–¹æ¡ˆ1: å¤šæ¨¡æ€åˆ†ç±»")
config = get_preset_config('balanced')  # ä½¿ç”¨å¹³è¡¡èåˆ
train_loader = create_default_dataloader("data/processed/multimodal_data.h5", 'train')
val_loader = create_default_dataloader("data/processed/multimodal_data.h5", 'val')

model = MultimodalClassifier(config)
trainer = MultimodalTrainer(model, config, train_loader, val_loader)
trainer.fit()

# === æ–¹æ¡ˆ2: çŸ¥è¯†è’¸é¦ (ä¿¡å·æ¨¡æ€æå‡) ===
print("æ–¹æ¡ˆ2: çŸ¥è¯†è’¸é¦ (68% â†’ 90%+)")
from src.models.knowledge_distillation import FeatureMimicryDistillation

# åˆ›å»ºè’¸é¦æ¨¡å‹
distill_model = FeatureMimicryDistillation(
    teacher_config=teacher_config,  # å›¾åƒæ¨¡æ€é…ç½®
    student_config=student_config,  # ä¿¡å·æ¨¡æ€é…ç½®  
    distillation_config=distill_config
)

# è’¸é¦è®­ç»ƒ
distill_criterion = DistillationLoss(alpha=0.3, beta=0.4, gamma=0.2, delta=0.1)
# ... è®­ç»ƒè¿‡ç¨‹ ...

# === æ–¹æ¡ˆ3: ä»…å›¾åƒåˆ†ç±» (æ¨è) ===
print("æ–¹æ¡ˆ3: å›¾åƒå•æ¨¡æ€ (95%+, æœ€å®ç”¨)")
config_img = get_preset_config('image_only')
model_img = MultimodalClassifier(config_img)
# ... ç®€å•é«˜æ•ˆ ...

print("é€‰æ‹©æ–¹æ¡ˆ:")
print("- æ–¹æ¡ˆ1: å®Œæ•´å¤šæ¨¡æ€ (96%, å¤æ‚)")
print("- æ–¹æ¡ˆ2: ä¿¡å·è’¸é¦ (90%+, ç ”ç©¶)")  
print("- æ–¹æ¡ˆ3: å›¾åƒå•æ¨¡æ€ (95%, æ¨è)")
"""
    return example


def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    import sys
    import torch
    import h5py
    import numpy as np
    
    info = {
        "python_version": sys.version,
        "pytorch_version": torch.__version__,
        "h5py_version": h5py.__version__,
        "numpy_version": np.__version__,
        "cuda_available": torch.cuda.is_available(),
        "cuda_version": torch.version.cuda if torch.cuda.is_available() else None,
        "device_count": torch.cuda.device_count() if torch.cuda.is_available() else 0
    }
    
    return info


def print_banner():
    """æ‰“å°é¡¹ç›®æ¨ªå¹…"""
    banner = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                            AplimC v{__version__}                          â•‘
â•‘               å¤šæ¨¡æ€è—»ç±»åˆ†ç±»ç³»ç»Ÿ + çŸ¥è¯†è’¸é¦æ¡†æ¶                    â•‘
â•‘                                                                  â•‘
â•‘  ğŸ§¬ æ”¯æŒæ¨¡æ€: Stokeså‚æ•° + è§å…‰ä¿¡å· + å¤šè§†å›¾å›¾åƒ                   â•‘
â•‘  ğŸ“Š æ•°æ®æ ¼å¼: HDF5 ä¼˜åŒ–å­˜å‚¨ (84GB â†’ é«˜æ•ˆè®¿é—®)                     â•‘
â•‘  ğŸ¯ åˆ†ç±»æ•°é‡: 12ç±»è—»ç±»                                            â•‘
â•‘  ğŸ“ˆ æ ·æœ¬æ€»æ•°: 21,007ä¸ª                                            â•‘
â•‘                                                                  â•‘
â•‘  ğŸ”„ æ–°åŠŸèƒ½ç‰¹æ€§:                                                   â•‘
â•‘     â€¢ çŸ¥è¯†è’¸é¦: å›¾åƒ(95%) â†’ ä¿¡å·(90%+)                          â•‘
â•‘     â€¢ å¹³è¡¡èåˆ: è§£å†³æ¨¡æ€å‚æ•°ä¸å¹³è¡¡é—®é¢˜                           â•‘
â•‘     â€¢ å¢å¼ºç¼–ç å™¨: å¤šå°ºåº¦ä¿¡å·ç‰¹å¾æå–                             â•‘
â•‘     â€¢ è·¨æ¨¡æ€æ³¨æ„åŠ›: æ™ºèƒ½ç‰¹å¾å…³è”                                 â•‘
â•‘                                                                  â•‘
â•‘  ğŸ“Š æ€§èƒ½åŸºå‡†:                                                     â•‘
â•‘     â€¢ å›¾åƒå•æ¨¡æ€: 95.59% (æ¨èæ–¹æ¡ˆ)                             â•‘
â•‘     â€¢ è§å…‰å•æ¨¡æ€: 66.96% â†’ è’¸é¦å90%+                           â•‘
â•‘     â€¢ å¤šæ¨¡æ€èåˆ: 96.41% (å¤æ‚åº¦â†‘, æ”¶ç›Šå¾®å°)                    â•‘
â•‘                                                                  â•‘
â•‘  éµå¾ªå¥¥å¡å§†å‰ƒåˆ€åŸç† - ç®€å•æœ‰æ•ˆä¼˜äºå¤æ‚                           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
    print(banner)


# æ¨¡å—åˆå§‹åŒ–æ—¶çš„æ£€æŸ¥
def _initialize():
    """æ¨¡å—åˆå§‹åŒ–"""
    try:
        # æ£€æŸ¥ä¾èµ–
        import torch
        import h5py
        import numpy as np
        
        # æ£€æŸ¥ç‰ˆæœ¬
        torch_version = tuple(map(int, torch.__version__.split('.')[:2]))
        if torch_version < (1, 8):
            import warnings
            warnings.warn(
                f"PyTorchç‰ˆæœ¬ {torch.__version__} å¯èƒ½å­˜åœ¨å…¼å®¹æ€§é—®é¢˜ï¼Œå»ºè®®ä½¿ç”¨ >=1.8.0",
                UserWarning
            )
        
        return True
        
    except ImportError as e:
        import warnings
        warnings.warn(f"ç¼ºå°‘ä¾èµ–é¡¹: {e}", ImportWarning)
        return False


# æ‰§è¡Œåˆå§‹åŒ–
_INITIALIZED = _initialize()


# ä¾¿æ·å‡½æ•°
def create_simple_classifier(hdf5_path: str, device: str = "auto"):
    """
    åˆ›å»ºç®€å•çš„åˆ†ç±»å™¨å®ä¾‹
    
    Args:
        hdf5_path: HDF5æ•°æ®æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ç±»å‹ ("auto", "cpu", "cuda:0", ç­‰)
        
    Returns:
        tuple: (model, train_loader, val_loader, config)
    """
    import torch
    
    # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # åˆ›å»ºé…ç½®
    config = get_preset_config('simple')
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = create_default_dataloader(
        hdf5_path=hdf5_path,
        split='train',
        batch_size=config.batch_size,
        balanced=True
    )
    
    val_loader = create_default_dataloader(
        hdf5_path=hdf5_path,
        split='val', 
        batch_size=config.batch_size,
        balanced=False
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = MultimodalClassifier(config).to(device)
    
    return model, train_loader, val_loader, config


def create_lightweight_classifier(hdf5_path: str, device: str = "auto"):
    """
    åˆ›å»ºè½»é‡çº§åˆ†ç±»å™¨å®ä¾‹ï¼ˆä»…ä¿¡å·æ¨¡æ€ï¼‰
    
    Args:
        hdf5_path: HDF5æ•°æ®æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ç±»å‹
        
    Returns:
        tuple: (model, train_loader, val_loader, config)
    """
    import torch
    
    # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
    if device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    
    # åˆ›å»ºè½»é‡çº§é…ç½®
    config = get_preset_config('lightweight')
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = create_default_dataloader(
        hdf5_path=hdf5_path,
        split='train',
        batch_size=config.batch_size,
        balanced=True
    )
    
    val_loader = create_default_dataloader(
        hdf5_path=hdf5_path,
        split='val',
        batch_size=config.batch_size, 
        balanced=False
    )
    
    # åˆ›å»ºæ¨¡å‹
    model = MultimodalClassifier(config).to(device)
    
    return model, train_loader, val_loader, config


# è°ƒè¯•ä¿¡æ¯
if __name__ == "__main__":
    print_banner()
    print("\nğŸ“‹ é¡¹ç›®ä¿¡æ¯:")
    for key, value in get_project_info().items():
        print(f"  {key}: {value}")
    
    print("\nğŸ”§ ç¯å¢ƒä¿¡æ¯:")
    env_info = check_environment()
    for key, value in env_info.items():
        print(f"  {key}: {value}")
    
    print("\nğŸš€ å¿«é€Ÿå¼€å§‹ç¤ºä¾‹:")
    print(quick_start_example())
